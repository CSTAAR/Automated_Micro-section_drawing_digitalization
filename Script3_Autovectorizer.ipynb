{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc32fb69-5577-489a-ae81-2868e362868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CS_10.tif\n",
      "Processing CS_100.tif\n",
      "Processing CS_1013.tif\n",
      "Processing CS_1014.tif\n",
      "Processing CS_1015.tif\n",
      "Processing CS_1026.tif\n",
      "Processing CS_1027.tif\n",
      "Processing CS_1028.tif\n",
      "Processing CS_103.tif\n",
      "Processing CS_104.tif\n",
      "Processing CS_1040.tif\n",
      "Processing CS_1041.tif\n",
      "Processing CS_1054.tif\n",
      "Processing CS_1055.tif\n",
      "Processing CS_11.tif\n",
      "Processing CS_12.tif\n",
      "Processing CS_122.tif\n",
      "Processing CS_126.tif\n",
      "Processing CS_127.tif\n",
      "Processing CS_128.tif\n",
      "Processing CS_14.tif\n",
      "Processing CS_145.tif\n",
      "Processing CS_15.tif\n",
      "Processing CS_150.tif\n",
      "Processing CS_151.tif\n",
      "Processing CS_152.tif\n",
      "Processing CS_153.tif\n",
      "Processing CS_154.tif\n",
      "Processing CS_170.tif\n",
      "Processing CS_176.tif\n",
      "Processing CS_178.tif\n",
      "Processing CS_179.tif\n",
      "Processing CS_181.tif\n",
      "Processing CS_197.tif\n",
      "Processing CS_207.tif\n",
      "Processing CS_208.tif\n",
      "Processing CS_209.tif\n",
      "Processing CS_210.tif\n",
      "Processing CS_2270.tif\n",
      "Processing CS_2277.tif\n",
      "Processing CS_2278.tif\n",
      "Processing CS_237.tif\n",
      "Processing CS_238.tif\n",
      "Processing CS_239.tif\n",
      "Processing CS_240.tif\n",
      "Processing CS_268.tif\n",
      "Processing CS_269.tif\n",
      "Processing CS_271.tif\n",
      "Processing CS_299.tif\n",
      "Processing CS_3.tif\n",
      "Processing CS_300.tif\n",
      "Processing CS_301.tif\n",
      "Processing CS_330.tif\n",
      "Processing CS_331.tif\n",
      "Processing CS_362.tif\n",
      "Processing CS_363.tif\n",
      "Processing CS_368.tif\n",
      "Processing CS_392.tif\n",
      "Processing CS_4.tif\n",
      "Processing CS_406.tif\n",
      "Processing CS_437.tif\n",
      "Processing CS_438.tif\n",
      "Processing CS_439.tif\n",
      "Processing CS_468.tif\n",
      "Processing CS_469.tif\n",
      "Processing CS_498.tif\n",
      "Processing CS_5.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\gis_py\\gis_py\\Lib\\site-packages\\shapely\\measurement.py:72: RuntimeWarning: invalid value encountered in distance\n",
      "  return lib.distance(a, b, **kwargs)\n",
      "C:\\Users\\filip\\gis_py\\gis_py\\Lib\\site-packages\\shapely\\measurement.py:72: RuntimeWarning: invalid value encountered in distance\n",
      "  return lib.distance(a, b, **kwargs)\n",
      "C:\\Users\\filip\\gis_py\\gis_py\\Lib\\site-packages\\shapely\\measurement.py:72: RuntimeWarning: invalid value encountered in distance\n",
      "  return lib.distance(a, b, **kwargs)\n",
      "C:\\Users\\filip\\gis_py\\gis_py\\Lib\\site-packages\\shapely\\measurement.py:72: RuntimeWarning: invalid value encountered in distance\n",
      "  return lib.distance(a, b, **kwargs)\n",
      "C:\\Users\\filip\\gis_py\\gis_py\\Lib\\site-packages\\shapely\\measurement.py:72: RuntimeWarning: invalid value encountered in distance\n",
      "  return lib.distance(a, b, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CS_500.tif\n",
      "Processing CS_532.tif\n",
      "Processing CS_533.tif\n",
      "Processing CS_55.tif\n",
      "Processing CS_566.tif\n",
      "Processing CS_567.tif\n",
      "Processing CS_568.tif\n",
      "Processing CS_598.tif\n",
      "Processing CS_599.tif\n",
      "Processing CS_6.tif\n",
      "Processing CS_629.tif\n",
      "Processing CS_630.tif\n",
      "Processing CS_658.tif\n",
      "Processing CS_659.tif\n",
      "Processing CS_660.tif\n",
      "Processing CS_687.tif\n",
      "Processing CS_688.tif\n",
      "Processing CS_689.tif\n",
      "Processing CS_715.tif\n",
      "Processing CS_716.tif\n",
      "Processing CS_717.tif\n",
      "Processing CS_743.tif\n",
      "Processing CS_77.tif\n",
      "Processing CS_8.tif\n",
      "Processing CS_80.tif\n",
      "Processing CS_9.tif\n",
      "Processing CS_966.tif\n",
      "Processing CS_967.tif\n",
      "Processing CS_968.tif\n",
      "Processing CS_983.tif\n",
      "Processing CS_984.tif\n",
      "Processing CS_985.tif\n",
      "Processing CS_998.tif\n",
      "Saved 100 polygons to C:\\Users\\filip\\Documents\\ARC_GIS_Dukovany\\shp_test\\VRSTVA.shp out of 100 rasters\n",
      "Time of processing: 19.40 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script 3: AutoVectorizer\n",
    "\n",
    "Description:\n",
    "This script takes a set of rectified and georeferenced section drawings (GeoTIFFs), detects horizontal stratigraphic lines\n",
    "using image processing techniques (template matching, edge detection, contour filtering), and converts them into vectorized \n",
    "polygons representing individual stratigraphic layers. The output is saved as a single shapefile.\n",
    "\n",
    "To run this script successfully, the user must define the following inputs:\n",
    "\n",
    "- input_folder:       path to the folder containing georeferenced section drawings (.tif)\n",
    "- template_path:      path to the image of the 1m scale bar used for template matching\n",
    "- output_path:        full path to the output shapefile (e.g., LAYER.shp)\n",
    "\n",
    "Other parameters, such as ROI width, filtering tolerances, and template resize factor, can be adjusted depending \n",
    "on the drawing style and quality of the inputs.\n",
    "\n",
    "Template Matching Note:\n",
    "If you want to apply this script to a different dataset, you will need to:\n",
    "- Create a new template image (typically a cropped image of the scale bar) from one of the new rasters.\n",
    "- Ensure that the template image is saved with the **exact same dimensions and scale ratio** as it appears in the dataset.\n",
    "- Adjust the `scale_template_resize_factor` and possibly other matching parameters to match the scale bar's appearance.\n",
    "- Incorrect or mismatched template size will cause inaccurate detection or failure of the workflow.\n",
    "\n",
    "Free tutorials on how template matching works in OpenCV are widely available (e.g., on YouTube), which can help you adapt this step to your own data.\n",
    "\n",
    "Notes:\n",
    "- The script assumes a consistent layout of scanned forms with a scale bar located in a predictable position.\n",
    "- Template matching is used to localize the drawing area for each image.\n",
    "- Output geometries include an attribute column \"Raster\" storing the source image name.\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import rasterio\n",
    "from matplotlib import pyplot as plt\n",
    "from shapely.geometry import LineString, Polygon\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from affine import Affine\n",
    "\n",
    "# ---------------------------------------------\n",
    "# USER SETTINGS (EDIT BEFORE RUNNING)\n",
    "# ---------------------------------------------\n",
    "input_folder = r\"C:\\path_to_input_folder\" # Path to folder containing GeoTIFFs with rectified and georeferenced section drawings\n",
    "template_path = r\"C:\\path_to_template\" # Path to the scale bar image used for template matching (must match size and style used in rectification)\n",
    "output_path = r\"C:\\path_to_output_shp\" # Path to the output shapefile where extracted polygons will be stored\n",
    "\n",
    "roi_width = 220 # Width of the Region of Interest (ROI) in pixels to be extracted next to the scale bar\n",
    "roi_y_offset_meters = 0.01 # Offset from the top of the ROI for placing the top line (used for closing polygons)\n",
    "pixel_y_tolerance = 10 # Tolerance (in pixels) for grouping points into horizontal lines based on Y-value similarity\n",
    "pixel_min_points = 15 # Minimum number of points required in a detected line to consider it valid\n",
    "pixel_min_x_span = 20 # Minimum horizontal span of a line in pixels to be considered valid\n",
    "scale_template_resize_factor = 2.35 # Resize factor for matching the template scale bar image to the scanned drawings\n",
    "\n",
    "# ---------------------------------------------\n",
    "# FUNCTIONS\n",
    "# ---------------------------------------------\n",
    "def display_polylines(polylines, title=\"Detected Polylines\"):\n",
    "    \"\"\"Display a list of polylines (LineString objects) using matplotlib.\"\"\"\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    for line in polylines:\n",
    "        x, y = zip(*line.coords)\n",
    "        plt.plot(x, y)\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "def template_matching(img, template_path):\n",
    "    \"\"\"Locate the scale bar in the image using template matching.\"\"\"\n",
    "    template = cv2.imread(template_path, 0)\n",
    "    template = cv2.resize(template, (int(template.shape[1] * scale_template_resize_factor),\n",
    "                                     int(template.shape[0] * scale_template_resize_factor)))\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    result = cv2.matchTemplate(img_gray, template, cv2.TM_CCOEFF_NORMED)\n",
    "    _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "    return max_loc, template.shape[::-1]\n",
    "\n",
    "def extract_roi(img, scale_loc, scale_size):\n",
    "    \"\"\"Extract the Region of Interest (ROI) to the right of the detected scale bar.\"\"\"\n",
    "    x, y = scale_loc\n",
    "    t_w, t_h = scale_size\n",
    "    roi_x = x + t_w\n",
    "    roi_y = y\n",
    "    roi = img[roi_y:roi_y + t_h, roi_x:roi_x + roi_width]\n",
    "    return roi, (roi_x, roi_y)\n",
    "\n",
    "def enhance_edges(roi):\n",
    "    \"\"\"Apply adaptive thresholding, morphological operations, and Canny edge detection to enhance line visibility.\"\"\"\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    processed = cv2.dilate(thresh, np.ones((2, 2), np.uint8), iterations=1)\n",
    "    processed = cv2.erode(processed, np.ones((2, 2), np.uint8), iterations=1)\n",
    "    edges = cv2.Canny(processed, 5, 100)\n",
    "    return edges\n",
    "\n",
    "def find_largest_y_group(points, y_tol):\n",
    "    \"\"\"Group points by similar Y values and return the largest group.\"\"\"\n",
    "    y_vals = np.array([pt[1] for pt in points])\n",
    "    bins = np.round(y_vals / y_tol) * y_tol\n",
    "    groups = [[pt for pt in points if round(pt[1] / y_tol) * y_tol == b] for b in np.unique(bins)]\n",
    "    return max(groups, key=len)\n",
    "\n",
    "def filter_horizontal_lines(contours, roi_origin, y_tol, min_pts, min_x_span):\n",
    "    \"\"\"Filter detected contours to identify horizontal lines representing stratigraphic boundaries.\"\"\"\n",
    "    roi_x, roi_y = roi_origin\n",
    "    lines = []\n",
    "    for c in contours:\n",
    "        points = [(int(p[0][0] + roi_x), int(p[0][1] + roi_y)) for p in c]\n",
    "        largest_group = find_largest_y_group(points, y_tol)\n",
    "        if len(largest_group) >= min_pts:\n",
    "            x_vals = [pt[0] for pt in largest_group]\n",
    "            x_range = np.ptp(x_vals)\n",
    "            if x_range >= min_x_span:\n",
    "                lines.append(LineString(largest_group))\n",
    "    return lines\n",
    "\n",
    "def transform_polylines(polylines, transform: Affine):\n",
    "    \"\"\"Apply a spatial transform (Affine) to a list of polylines.\"\"\"\n",
    "    return [LineString([transform * pt for pt in line.coords]) for line in polylines]\n",
    "\n",
    "def remove_duplicate_points(points):\n",
    "    \"\"\"Remove consecutive duplicate points from a list of (x, y) coordinates.\"\"\"\n",
    "    unique_points = []\n",
    "    for point in points:\n",
    "        if not unique_points or unique_points[-1] != point:\n",
    "            unique_points.append(point)\n",
    "    return unique_points\n",
    "\n",
    "def create_polygons_from_polylines(polylines, roi_top, roi_left_x, roi_right_x):\n",
    "    \"\"\"Create closed polygons between adjacent horizontal lines to represent stratigraphic units.\"\"\"\n",
    "    polygons = []\n",
    "\n",
    "    sorted_polylines = sorted(polylines, key=lambda line: np.mean([y for x, y in line.coords]), reverse=True)\n",
    "\n",
    "    if len(sorted_polylines) < 2:\n",
    "        print(\"Not enough polylines to build polygons.\")\n",
    "        return polygons\n",
    "\n",
    "    # 1. Sort x within lines\n",
    "    for i, line in enumerate(sorted_polylines):\n",
    "        sorted_coords = sorted(line.coords, key=lambda p: p[0])\n",
    "        sorted_polylines[i] = LineString(sorted_coords)\n",
    "\n",
    "    # 2. Clip polylines to roi_x range\n",
    "    new_polylines = []\n",
    "    for line in sorted_polylines:\n",
    "        coords = list(line.coords)\n",
    "        mid_coords = [(x, y) for x, y in coords if roi_left_x < x < roi_right_x]\n",
    "        if not mid_coords:\n",
    "            y_mean = np.mean([y for x, y in coords])\n",
    "            new_coords = [(roi_left_x, y_mean), (roi_right_x, y_mean)]\n",
    "        else:\n",
    "            y_start = np.mean([y for x, y in coords if x <= roi_left_x]) if any(x <= roi_left_x for x, _ in coords) else np.mean([y for _, y in mid_coords])\n",
    "            y_end = np.mean([y for x, y in coords if x >= roi_right_x]) if any(x >= roi_right_x for x, _ in coords) else np.mean([y for _, y in mid_coords])\n",
    "            new_coords = [(roi_left_x, y_start)] + mid_coords + [(roi_right_x, y_end)]\n",
    "        new_polylines.append(LineString(new_coords))\n",
    "\n",
    "    # 3. Replace top line\n",
    "    new_polylines[0] = LineString([(roi_left_x, roi_top), (roi_right_x, roi_top)])\n",
    "\n",
    "    # 4. Create polygons\n",
    "    for i in range(len(new_polylines) - 1):\n",
    "        top = new_polylines[i]\n",
    "        bottom = new_polylines[i + 1]\n",
    "        poly_coords = (\n",
    "            [top.coords[0]] +\n",
    "            list(top.coords) +\n",
    "            [top.coords[-1], bottom.coords[-1]] +\n",
    "            list(bottom.coords)[::-1] +\n",
    "            [bottom.coords[0], top.coords[0]]\n",
    "        )\n",
    "        poly_coords = remove_duplicate_points(poly_coords)\n",
    "        polygons.append(Polygon(poly_coords))\n",
    "\n",
    "    return polygons\n",
    "\n",
    "def remove_close_duplicate_lines(polylines, distance_threshold=0.05):\n",
    "    \"\"\" Remove redundant polylines that are close to each other.\"\"\"\n",
    "    kept_lines = []\n",
    "    discarded = set()\n",
    "\n",
    "    for i, line in enumerate(polylines):\n",
    "        if i in discarded or line.is_empty or not line.is_valid:\n",
    "            continue\n",
    "\n",
    "        span_i = np.ptp([x for x, y in line.coords])\n",
    "        for j in range(i + 1, len(polylines)):\n",
    "            other = polylines[j]\n",
    "            if j in discarded or other.is_empty or not other.is_valid:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if line.distance(other) < distance_threshold:\n",
    "                    span_j = np.ptp([x for x, y in other.coords])\n",
    "                    if span_i >= span_j:\n",
    "                        discarded.add(j)\n",
    "                    else:\n",
    "                        discarded.add(i)\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not compute distance between line {i} and {j}: {e}\")\n",
    "\n",
    "        if i not in discarded:\n",
    "            kept_lines.append(line)\n",
    "\n",
    "    return kept_lines\n",
    "\n",
    "# ---------------------------------------------\n",
    "# MAIN PROCESS\n",
    "# ---------------------------------------------\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    all_geoms = []\n",
    "    all_names = []\n",
    "    raster_count = 0\n",
    "    \n",
    "    all_polygons = []\n",
    "    #crs_used = crs\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(\".tif\"):\n",
    "            raster_count += 1\n",
    "            tif_path = os.path.join(input_folder, filename)\n",
    "            print(f\"Processing {filename}\")\n",
    "\n",
    "            with rasterio.open(tif_path) as src:\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "                crs_used = crs\n",
    "                img_data = src.read()\n",
    "                img = np.transpose(img_data, (1, 2, 0)) if img_data.shape[0] == 3 else img_data[0]\n",
    "                if len(img.shape) == 2:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            scale_loc, scale_size = template_matching(img, template_path)\n",
    "            roi, roi_origin = extract_roi(img, scale_loc, scale_size)\n",
    "            _, roi_top = transform * (roi_origin[0], roi_origin[1])\n",
    "            roi_left_x, _ = transform * (roi_origin[0], roi_origin[1])\n",
    "            roi_right_x, _ = transform * (roi_origin[0] + roi_width, roi_origin[1])\n",
    "            edges = enhance_edges(roi)\n",
    "            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            pixel_polylines = filter_horizontal_lines(contours, roi_origin, pixel_y_tolerance, pixel_min_points, pixel_min_x_span)\n",
    "            real_polylines = transform_polylines(pixel_polylines, transform)\n",
    "            real_polylines = remove_close_duplicate_lines(real_polylines, distance_threshold=0.02)\n",
    "            polygons = create_polygons_from_polylines(real_polylines, roi_top, roi_left_x, roi_right_x)\n",
    "            if polygons:\n",
    "                raster_name = os.path.splitext(filename)[0]\n",
    "                gdf = gpd.GeoDataFrame({\n",
    "                    \"geometry\": polygons,\n",
    "                    \"Raster\": [raster_name] * len(polygons)\n",
    "                    }, crs=crs)\n",
    "            all_polygons.append(gdf)\n",
    "\n",
    "    # --- Save all to one shapefile ---\n",
    "    if all_polygons:\n",
    "        final_gdf = gpd.GeoDataFrame(pd.concat(all_polygons, ignore_index=True))\n",
    "        final_gdf.to_file(output_path)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Saved {len(all_polygons)} polygons to {output_path} out of {raster_count} rasters\")\n",
    "        print(f\"Time of processing: {elapsed:.2f} seconds\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No polygons generated.\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344cc93-d5bd-4c1e-adb7-e4f8bb4f0433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
